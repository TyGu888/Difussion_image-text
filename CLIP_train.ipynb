{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from transformers import CLIPProcessor, CLIPModel,CLIPImageProcessor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor =  CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor =  CLIPImageProcessor.from_pretrained(\".\\clip_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./clip_processor/preprocessor_config.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_processor.save_pretrained('./clip_processor/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor =  CLIPImageProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "BATCHSIZE=128\n",
    "SAVE_OPT_CKP = True\n",
    "SAVE_MODEL_CKP = True\n",
    "UNFREEZE_START = 20 # set it to lower number when significantly more samples are included.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "run_name = f'clip224-l18'\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(pred, target):\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    output = -cos(pred, target).mean()\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_train_test_split():\n",
    "    \"\"\"add your image paths and embedding labels here\"\"\"\n",
    "    #encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    #train data is df_filtered.csv, train_images'path is in column 'filepath', train_labels is in column 'prompt'\n",
    "    #test data is test.csv, test_images'path is in column 'image_path', test_labels is in column 'Prompt'\n",
    "    \n",
    "    train_data = pd.read_csv('df_filtered.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "    train_images = train_data['filepath']\n",
    "    #train_labels = train_data['prompt']\n",
    "    #train_labels=encoder.encode(train_data['prompt'], batch_size=512, show_progress_bar=True, device=\"cuda\", convert_to_tensor=True)\n",
    "    test_images = test_data['image_path']\n",
    "    #test_labels = test_data['Prompt']\n",
    "    #test_labels=encoder.encode(test_data['Prompt'], batch_size=512, show_progress_bar=True, device=\"cuda\", convert_to_tensor=True)\n",
    "    #del encoder\n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    train_labels = torch.load('train_targets.pt')\n",
    "    test_labels = torch.load('test_targets.pt')\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        clip = AutoModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.vision = clip.vision_model\n",
    "        self.fc = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vision(x)['pooler_output']\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def load_pretrained_model():\n",
    "    model = Net()\n",
    "\n",
    "    trainable_model_weights = False\n",
    "    for name, child in model.named_children():\n",
    "        if name == 'vision':\n",
    "            for pn, p in child.named_parameters():\n",
    "                if str(UNFREEZE_START) in pn:\n",
    "                    \"\"\"start unfreezing layer , the weights are trainable\"\"\"\n",
    "                    trainable_model_weights = True\n",
    "                p.requires_grad = trainable_model_weights\n",
    "                if p.requires_grad:\n",
    "                    print(f\"{pn} is set to be trainable.\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "class IMGDataset:\n",
    "    def __init__(self, image_paths, targets, clip_processor=clip_processor):\n",
    "        self.images = image_paths\n",
    "        self.labels = targets\n",
    "        self.input_processor = clip_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.images[item])\n",
    "        image = self.input_processor(image)['pixel_values']\n",
    "        #image = torch.tensor(image)\n",
    "        target = self.labels[item]\n",
    "        return image[0], target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836a50c140d7440b9d32397d09b26271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8fd4cdc95345bb9c31a4d02a83124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_24012\\1274190032.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_targets = torch.tensor(train_targets)\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_24012\\1274190032.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_targets = torch.tensor(test_targets)\n"
     ]
    }
   ],
   "source": [
    "#save train_targets and test_targets, so that we can use them in the future\n",
    "train_images, train_targets, test_images, test_targets = get_train_test_split()\n",
    "train_targets = torch.tensor(train_targets)\n",
    "test_targets = torch.tensor(test_targets)\n",
    "torch.save(train_targets, 'train_targets.pt')\n",
    "torch.save(test_targets, 'test_targets.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 73718, train size: 733683\n",
      "encoder.layers.20.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.bias is set to be trainable.\n",
      "post_layernorm.weight is set to be trainable.\n",
      "post_layernorm.bias is set to be trainable.\n",
      "load\n",
      "Entering epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5732 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"main training\"\"\"\n",
    "Path(f\"./{run_name}\").mkdir(exist_ok=True)\n",
    "\n",
    "NEPOCH=25\n",
    "BestEpoch=0\n",
    "BestSim = 0\n",
    "train_images, train_targets, test_images, test_targets = get_train_test_split()\n",
    "\n",
    "print(f\"test size: {len(test_images)}, train size: {len(train_images)}\")\n",
    "#train_targets=np.loadtxt('train_targets.csv', delimiter=',')\n",
    "#test_targets=np.loadtxt('test_targets.csv', delimiter=',')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "nn_model = load_pretrained_model()\n",
    "#nn_model = torch.compile(nn_model)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, nn_model.parameters()), lr=1e-4, fused=True)\n",
    "optimizer.zero_grad()\n",
    "test_dataloader = DataLoader(dataset=IMGDataset(test_images, test_targets),\n",
    "                                batch_size=BATCHSIZE, shuffle=False, num_workers=6)\n",
    "train_dataloader = DataLoader(dataset=IMGDataset(train_images, train_targets),\n",
    "                                batch_size=BATCHSIZE, shuffle=True, num_workers=6)\n",
    "print('load')\n",
    "\n",
    "for epoch in range(NEPOCH):\n",
    "    epoch_loss = 0\n",
    "    print(f'Entering epoch {epoch}')\n",
    "    for s, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "        batch_images, batch_targets = batch_data\n",
    "\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "\n",
    "        pred = nn_model(batch_images)\n",
    "        cosine_loss = cosine_similarity_loss(pred, batch_targets)\n",
    "        loss = cosine_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += -cosine_loss.item()\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    print(f\"epoch: {epoch}, training loss: {epoch_loss}\")\n",
    "    \n",
    "    \"\"\"test loss\"\"\"\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_targets in tqdm(test_dataloader):\n",
    "            batch_images, batch_targets = batch_images.to(device), batch_targets.to(device)\n",
    "            pred = nn_model(batch_images)\n",
    "            loss = -cosine_similarity_loss(pred, batch_targets)\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(test_dataloader)\n",
    "    print(f\"epoch: {epoch}, test loss: {epoch_loss}\")\n",
    "\n",
    "    if epoch_loss > BestSim:\n",
    "        BestSim = epoch_loss\n",
    "        BestEpoch = epoch + 1\n",
    "        print(f\"save best model at {BestSim} with epoch {BestEpoch}\")\n",
    "        if SAVE_MODEL_CKP:\n",
    "            torch.save(nn_model.state_dict(), f\"{run_name}.pt\")\n",
    "        if SAVE_OPT_CKP:\n",
    "            torch.save(optimizer.state_dict(), f\"{run_name}_opt.pt\")\n",
    "\n",
    "    if epoch - 3 > BestEpoch:\n",
    "        print(f\"early stop at {epoch+1} with best epoch {BestEpoch} and test similarity {BestSim}.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 73718, train size: 733683\n",
      "encoder.layers.20.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.20.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.20.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.20.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.21.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.21.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.21.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.22.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.22.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.22.layer_norm2.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.k_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.v_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.q_proj.bias is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.weight is set to be trainable.\n",
      "encoder.layers.23.self_attn.out_proj.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc1.bias is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.weight is set to be trainable.\n",
      "encoder.layers.23.mlp.fc2.bias is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.weight is set to be trainable.\n",
      "encoder.layers.23.layer_norm2.bias is set to be trainable.\n",
      "post_layernorm.weight is set to be trainable.\n",
      "post_layernorm.bias is set to be trainable.\n",
      "load\n",
      "Entering epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5732 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"main training\"\"\"\n",
    "    Path(f\"./{run_name}\").mkdir(exist_ok=True)\n",
    "\n",
    "    NEPOCH=25\n",
    "    BestEpoch=0\n",
    "    BestSim = 0\n",
    "    train_images, train_targets, test_images, test_targets = get_train_test_split()\n",
    "\n",
    "    print(f\"test size: {len(test_images)}, train size: {len(train_images)}\")\n",
    "    #train_targets=np.loadtxt('train_targets.csv', delimiter=',')\n",
    "    #test_targets=np.loadtxt('test_targets.csv', delimiter=',')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    nn_model = load_pretrained_model()\n",
    "    #nn_model = torch.compile(nn_model)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, nn_model.parameters()), lr=1e-4, fused=True)\n",
    "    optimizer.zero_grad()\n",
    "    test_dataloader = DataLoader(dataset=IMGDataset(test_images, test_targets),\n",
    "                                 batch_size=BATCHSIZE, shuffle=False, num_workers=4)\n",
    "    train_dataloader = DataLoader(dataset=IMGDataset(train_images, train_targets),\n",
    "                                 batch_size=BATCHSIZE, shuffle=True, num_workers=4)\n",
    "    print('load')\n",
    "\n",
    "    for epoch in range(NEPOCH):\n",
    "        epoch_loss = 0\n",
    "        print(f'Entering epoch {epoch}')\n",
    "        for s, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "            batch_images, batch_targets = batch_data\n",
    "\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "\n",
    "            pred = nn_model(batch_images)\n",
    "            cosine_loss = cosine_similarity_loss(pred, batch_targets)\n",
    "            loss = cosine_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += -cosine_loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        print(f\"epoch: {epoch}, training loss: {epoch_loss}\")\n",
    "        \n",
    "        \"\"\"test loss\"\"\"\n",
    "        epoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_targets in tqdm(test_dataloader):\n",
    "                batch_images, batch_targets = batch_images.to(device), batch_targets.to(device)\n",
    "                pred = nn_model(batch_images)\n",
    "                loss = -cosine_similarity_loss(pred, batch_targets)\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(test_dataloader)\n",
    "        print(f\"epoch: {epoch}, test loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss > BestSim:\n",
    "            BestSim = epoch_loss\n",
    "            BestEpoch = epoch + 1\n",
    "            print(f\"save best model at {BestSim} with epoch {BestEpoch}\")\n",
    "            if SAVE_MODEL_CKP:\n",
    "                torch.save(nn_model.state_dict(), f\"{run_name}.pt\")\n",
    "            if SAVE_OPT_CKP:\n",
    "                torch.save(optimizer.state_dict(), f\"{run_name}_opt.pt\")\n",
    "\n",
    "        if epoch - 3 > BestEpoch:\n",
    "            print(f\"early stop at {epoch+1} with best epoch {BestEpoch} and test similarity {BestSim}.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
